{"cells":[{"cell_type":"markdown","metadata":{"id":"gJOVeSQi_HqJ"},"source":["# Module 2: Data wrangling using `pandas`\n","\n","## Overview: Wrangle a messy geothermal dataset\n","Messy datasets are the norm in geoscience - this notebook will take you through python code very commonly used to import and wrangle some messy geothermal data. We will use `pandas`, a fast, powerful, flexible, and easy-to-use data analysis and manipulation tool.\n","\n","For questions on this notebook, ask them on the [GEOL 557 slack](https://join.slack.com/t/minesgeo/shared_invite/zt-cqawm4lu-Zcfpf4mBLwjnksY6_umlKA)<a href=\"https://join.slack.com/t/minesgeo/shared_invite/zt-cqawm4lu-Zcfpf4mBLwjnksY6_umlKA\">\n","<img src=\"https://cdn.brandfolder.io/5H442O3W/as/pl546j-7le8zk-ex8w65/Slack_RGB.svg\" alt=\"Go to the GEOl 557 slack\" width=\"100\">\n","</a>\n","\n","## Instructions\n","Work through this notebook - there will be several places where you need to fill-in-the-blank or write some code into an open cell. When you are finished, make sure to use the Colab menu (not the browser-level menu) to do the following:\n","- Expand all the sections - in the Colab menu, choose View --> Expand sections) \n","- Save the notebook as a pdf, again using the Colab menu, using File --> Print --> Save as PDF. \n","\n","--- \n","## Course\n","**GEOL 557 Earth Resource Data Science I: Fundamentals**. GEOL 557 forms part 2 of the four-part course series for the \"Earth Resource Data Science\" online graduate certificate at Mines - [learn more about the certificate here](https://online.mines.edu/er/)\n","\n","Notebook created by **Zane Jobe** and **Thomas Martin**, [CoRE research group](https://core.mines.edu), Colorado School of Mines\n","\n","[![Twitter URL](https://img.shields.io/twitter/url/https/twitter.com/ZaneJobe.svg?style=social&label=Follow%20%40ZaneJobe)](https://twitter.com/ZaneJobe)\n","and [![Twitter URL](https://img.shields.io/twitter/url/https/twitter.com/ThomasM_geo.svg?style=social&label=Follow%20%40ThomasM_geo)](https://twitter.com/ThomasM_geo) on Twitter "]},{"cell_type":"markdown","metadata":{"id":"sOio4icDzWKo"},"source":["# First, let's mount Google Drive and import `pandas`\n","We can import `pandas` normally because Google pre-loaded them into Colab. Google does not pre-load everything that you need, sometimes you might need to install it. If `pandas` wasn't a common library, you would have to install it using `pip` or some other method, but `pandas` is used everyday by millions of people. As an aside, the syntax to install a library would be `!pip install pandas`\n","\n","---------------------"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1wGxu-9R_HqR"},"outputs":[],"source":["import pandas as pd # this imports pandas to this notebook\n","import numpy as np\n","\n","# these next two things shuoldnt need to be changed if you set up your Google Drive folder correctly (see Module 1)\n","folder_path = '../1_data/'\n","file_name = 'GeothermalPowerPlants_wikipedia.csv' # file name"]},{"cell_type":"markdown","metadata":{"id":"NaN2luxZ3Kj5"},"source":["## Let's load in a dataset\n","We load it with the pandas `read_csv` function into a [DataFrame](https://pandas.pydata.org/pandas-docs/stable/getting_started/intro_tutorials/01_table_oriented.html). We will name this DataFrame `df` and then preview it with the `head` function, giving us a visual view of the dataframe. Just like Excel, except `pandas` is so, so much better and you will see why soon. \n","\n","This table is from Wikipedia, from [here](https://en.wikipedia.org/wiki/List_of_geothermal_power_stations_in_the_United_States). I used https://wikitable2csv.ggor.de/ to make a csv from the table! Pretty nifty, but no guarantees on accuracy. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H6AyOjCX3KrN"},"outputs":[],"source":["\n","df= pd.read_csv(folder_path + file_name) # uses pandas to read in the csv as a 'DataFrame' called df\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"O3P0_Ms13K-A"},"source":["By default, head looks at the first five rows using `df.head()`. Or you can specify how many rows to look at by putting a number in the parentheses (e.g., `df.head(2)`). You can also use `tail()` to look at the last five rows. \n","\n","You can also just list the column names using `df.columns` or `df.keys()`. We will use keys, as they behave similarly to keys in a python dict (i.e., dict = {key: value}). To only see the 3rd key, use `df.keys()[2]`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZNNkt0EJHjRM"},"outputs":[],"source":["df.keys()"]},{"cell_type":"markdown","metadata":{"id":"dOrbPCpKIge1"},"source":["OK, so we can see how the DataFrame is set up, and now we need to figure out what exactly is in each column. For this, we use `info()`, which gives a summary of the dataframe, and importantly, what their `dtypes` are:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EQ6U0oxg3LDL"},"outputs":[],"source":["print(df.info())\n","np.shape(df)"]},{"cell_type":"markdown","metadata":{"id":"KYOewm5IEoKe"},"source":["So, there are 31 rows (you could also find this by using `len(df)`) and 8 columns `(len(df.keys())`. \n","\n","The Non-Null Count is how many values are not empty (or have Not-a-Number or NaN values). \n","\n","The Dtype is quite important for using and plotting the data. Notice that every column except for `Year` has a dtype of `object`, which isn't great, as object is the pandas-version of `string`. Year is listed as an int, which is good (i.e., Year shouldn't be a float or a string/object).\n","\n","This probably means that there are weird values in some of the columns. This is pretty typical for datasets inputted manually (by someone typing in the numbers), outputted from an instrument, or OCRed. \n","\n","Let's take a look at one of the columns that's listed as an object dtype and should be a string, like the Name of the power plant. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lt1Q2vF4Dtsc"},"outputs":[],"source":["df.Name"]},{"cell_type":"markdown","metadata":{"id":"BY3HUYQYCRCH"},"source":["Now let's look at another column that's listed as object, but we know by looking at the df.head() that it should be a number (either `int` or `float`)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eayqJsJlCRHD"},"outputs":[],"source":["df['Capacity(MW)'] # We have to use df['column name'] instead of df.column_name if the column has spaces or weird characters"]},{"cell_type":"markdown","metadata":{"id":"Hha8qYae3LKs"},"source":["Hmm, not good. One of the values has a comma (row 9), and two values  (rows 25, 30) have other weird text.\n","\n","For this size of dataset, it may just be easier to do what we are going to do in Excel ðŸ˜±. But, if this dataset was 5x-1000x larger, or you needed to do this to several hundred Excel files that were formatted the same way (which is typical when getting data from instruments or service companies), it is much easier to clean up using some python tools. Remember, ***automate the boring stuff!***\n","\n","First, let's remove the commas from the dataframe.\n","\n","Commonly, data you import from Excel will have random extra characters like commas - this happens when an Excel column is a number stored as text, so 1000 is 1,000, and python/pandas doesn't like that. Remember [this article?](https://www.forbes.com/sites/salesforce/2014/09/13/sorry-spreadsheet-errors/#32d3148856ab)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-aEMPTr43LnB"},"outputs":[],"source":["df.replace(',','', regex=True, inplace=True) # searches the whole DataFrame and replaces commas with nothing\n","df['Capacity(MW)']"]},{"cell_type":"markdown","metadata":{"id":"stR3jYRjJfAH"},"source":["Progress! The comma went away. \n","\n","You can also use something like this if you have a really messy dataset (taken from [here](https://medium.com/analytics-vidhya/simplify-your-dataset-cleaning-with-pandas-75951b23568e)):\n","\n","     spec_chars = [\"!\",'\"',\"#\",\"%\",\"&\",\"'\",\"(\",\")\",\n","              \"*\",\"+\",\",\",\"-\",\".\",\"/\",\":\",\";\",\"<\",\n","              \"=\",\">\",\"?\",\"@\",\"[\",\"\\\\\",\"]\",\"^\",\"_\",\n","              \"`\",\"{\",\"|\",\"}\",\"~\",\"â€“\"]\n","     for char in spec_chars:\n","       df['column name'] = df['column name'].str.replace(char, '')\n","\n","Be careful doing this type of thing, as it can really mess things up too, but it can save you a lifetime of manual Excel find-and-replace agony.\n","\n","Now, it kind of sucks to have to type `Capacity(MW)', so let's change the names of two of the columns so they are easier to type, and drop the Ref column (we don't care about it for now)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BiHYG81y3LRi"},"outputs":[],"source":["df = df.rename(columns={\"Capacity(MW)\": \"Capacity_MW\", \"Annual Generation(GWh)\": \"AG_GWh\"})\n","df = df.drop(columns=['Ref']) # gets rid of the column `Ref`\n","df.replace(',','', regex=True, inplace=True)\n","\n","df.Capacity_MW"]},{"cell_type":"markdown","metadata":{"id":"WnVbRx7BHTDh"},"source":["Ahh, much nicer. If you try to call that column by it's old name, you get a KeyError, which is saying that there isn't a key that's called \"Capacity(MW)\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nJ7m94YdKdsJ"},"outputs":[],"source":["# df['Capacity(MW)'] "]},{"cell_type":"markdown","metadata":{"id":"vzPGkBwOQYf0"},"source":["Now let's get rid of those other issues in the Capacity_MW column, where there is text after the numbers. We will use a Series function on the column, see details [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.html?highlight=series%20str#pandas.Series.str). Specifically, we will `split` on the space, and only keep the values before the space:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E24XeCdVKhTF"},"outputs":[],"source":["df.Capacity_MW = df.Capacity_MW.str.split(\" \", expand=True)[0] # splitting on the space delimiter and only keeping the first result [0]\n","df.Capacity_MW = pd.to_numeric(df.Capacity_MW) # Changing the type to numeric\n","print(df.Capacity_MW.dtype)\n","df.Capacity_MW"]},{"cell_type":"markdown","metadata":{"id":"MQBDdTI7Slk4"},"source":["OK, now let's fix some other columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G-QXhlp-XL17"},"outputs":[],"source":["df.AG_GWh = df.AG_GWh.str.split(\" \", expand=True)[0]\n","df.AG_GWh = pd.to_numeric(df.AG_GWh)\n","df.AG_GWh"]},{"cell_type":"markdown","metadata":{"id":"0pkibWSPXSEw"},"source":["For the Lat-Lon data, there are two ways to clean this column up(well, probably several, but we will only discuss two).\n","\n","The first is to split it out to get only the decimal degrees part, and then separate the decimal degree into lat and lon values, creating new Series for each (and multiplying the lon by -1 to account for them all being West values):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4-D9V9LiXeka"},"outputs":[],"source":["df.LocationDD = df.Location.str.split(r\" / \", expand=True)[1]\n","df.LocationDD.head(2) # showing just the first two entries\n","\n","#df.latDD_N  = df.LocationDD.str.split(r\" *\", expand=True)[0] \n","#df.longDD_W = df.LocationDD.str.split(r\" *\", expand=True)[1]\n","df.latDD_N  = df.LocationDD.str.split(r\" \", expand=True)[0] \n","df.longDD_W = df.LocationDD.str.split(r\" \", expand=True)[1]\n","\n","df.latDD_N.replace('Â°N','', regex=True, inplace=True) # replacing the degree symbol with a space\n","df.longDD_W.replace('Â°W','', regex=True, inplace=True)\n","\n","df.latDD_N.values\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cFfPe_qTEHt0"},"outputs":[],"source":["df.latDD_N.replace('\\ufeff','', regex=True, inplace=True) # doing more replacing\n","df.longDD_W.replace('\\ufeff','', regex=True, inplace=True)\n","\n","df['latDD_N'] = pd.to_numeric(df.latDD_N)\n","df['longDD_W'] = pd.to_numeric(df.longDD_W)\n","\n","df.latDD_N = pd.to_numeric(df.latDD_N)\n","df.longDD_W = pd.to_numeric(df.longDD_W)\n","\n","# Because it is in the west, easier to put a negative\n","df.longDD_W = df.longDD_W*-1 \n","df.longDD_W.values"]},{"cell_type":"markdown","metadata":{"id":"eFcyuH0HX4cQ"},"source":["The other is to use regex and applymap to each element of a Series"]},{"cell_type":"markdown","metadata":{"id":"COfazlN2SluR"},"source":["from https://realpython.com/python-data-cleaning-numpy-pandas/\n","\n","DO THE APPLYMAP too!!!\n","\n","r'^(\\d{4})'\n","extr = df['Date of Publication'].str.extract(r'^(\\d{4})', expand=False)\n","df['Date of Publication'] = pd.to_numeric(extr)\n","\n","The regular expression above is meant to find any four digits at the beginning of a string, which suffices for our case. The above is a raw string (meaning that a backslash is no longer an escape character), which is standard practice with regular expressions.\n","\n","The \\d represents any digit, and {4} repeats this rule four times. The ^ character matches the start of a string, and the parentheses denote a capturing group, which signals to Pandas that we want to extract that part of the regex. (We want ^ to avoid cases where [ starts off the string.)\n","\n","Letâ€™s see what happens when we run this regex across our dataset:\n","\n","https://realpython.com/regex-python/\n","\n","This results in about one in every ten values being missing, which is a small price to pay for now being able to do computations on the remaining valid values:\n","df['Date of Publication'].isnull().sum() / len(df)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yN7YtbZlJ9KW"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"XlaiUa3_Sl0H"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"awZZsL_USl5k"},"outputs":[],"source":["df.groupby(['State']).size()"]},{"cell_type":"markdown","metadata":{"id":"u6koy9HISl-x"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lXMLAgeDSmJL"},"outputs":[],"source":["df.groupby(['State']).sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_XNdFrp7YHZ8"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"QefN88gso73j"},"source":["![I'm done](https://media1.tenor.com/images/bfc7eb4615c496283ba14f079c2e499e/tenor.gif?itemid=10583001)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4hxIWy7Uo-QT"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
