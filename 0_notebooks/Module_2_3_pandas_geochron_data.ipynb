{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"gJOVeSQi_HqJ"},"source":["# Module 2: Data wrangling using `pandas`\n","\n","## Overview: Clean vs. dirty geochronology datasets\n","This notebook will take you through python code to import, sort, and make some very basic plots of some U-Pb detrital zircon geochronology data using `pandas`. We will look at one nicely cleaned dataset, and anothrer dataset that is messier, and needs some wrangling before we can play with it.\n","\n","For questions on this notebook, ask them on the [GEOL 557 slack](https://join.slack.com/t/minesgeo/shared_invite/zt-cqawm4lu-Zcfpf4mBLwjnksY6_umlKA)<a href=\"https://join.slack.com/t/minesgeo/shared_invite/zt-cqawm4lu-Zcfpf4mBLwjnksY6_umlKA\">\n","<img src=\"https://cdn.brandfolder.io/5H442O3W/as/pl546j-7le8zk-ex8w65/Slack_RGB.svg\" alt=\"Go to the GEOl 557 slack\" width=\"100\">\n","</a>\n","\n","## Instructions\n","Work through this notebook - there will be several places where you need to fill-in-the-blank or write some code into an open cell. When you are finished, make sure to use the Colab menu (not the browser-level menu) to do the following:\n","- Expand all the sections - in the Colab menu, choose View --> Expand sections) \n","- Save the notebook as a pdf, again using the Colab menu, using File --> Print --> Save as PDF. \n","\n","--- \n","## Course\n","**GEOL 557 Earth Resource Data Science I: Fundamentals**. GEOL 557 forms part 2 of the four-part course series for the \"Earth Resource Data Science\" online graduate certificate at Mines - [learn more about the certificate here](https://online.mines.edu/er/)\n","\n","Notebook created by **Zane Jobe** and **Thomas Martin**, [CoRE research group](https://core.mines.edu), Colorado School of Mines\n","\n","[![Twitter URL](https://img.shields.io/twitter/url/https/twitter.com/ZaneJobe.svg?style=social&label=Follow%20%40ZaneJobe)](https://twitter.com/ZaneJobe)\n","and [![Twitter URL](https://img.shields.io/twitter/url/https/twitter.com/ThomasM_geo.svg?style=social&label=Follow%20%40ThomasM_geo)](https://twitter.com/ThomasM_geo) on Twitter \n","\n","# TO DO: change path to GEOL_557"]},{"cell_type":"code","metadata":{"id":"1wGxu-9R_HqR"},"source":["import pandas as pd # this imports pandas to this notebook\n","import numpy as np\n","from matplotlib import pyplot as plt\n","\n","from google.colab import drive # this mounts Google Drive to this notebook\n","drive.mount('/content/gdrive')\n","\n","# these next two things shuoldnt need to be changed if you set up your Google Drive folder correctly (see Module 1)\n","folder_path = 'gdrive/My Drive/GEOL557_F22/data/' # makes a path\n","file_name = 'Sharman_ExampleDataset_1.xlsx' # file name"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NaN2luxZ3Kj5"},"source":["## Let's load in a dataset\n","The data we want is located here https://github.com/grsharman/detritalPy/blob/master/detritalPy/example-data/ExampleDataset_1.xlsx\n","\n","It is downloaded and saved in the Google Drive folder. \n","\n","We load it with the pandas `read_xls` function into a DataFrame called `df`:"]},{"cell_type":"code","metadata":{"id":"H6AyOjCX3KrN"},"source":["df=pd.read_excel(folder_path + file_name, sheet_name='ZrUPb') # uses pandas to read in the csv as a 'DataFrame' called df\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O3P0_Ms13K-A"},"source":["Nice! Let's make sure the dtypes are correct:"]},{"cell_type":"code","metadata":{"id":"awZZsL_USl5k"},"source":["df.info()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lV70MjqW9f50"},"source":["Excellent - I wish all data files were formatted like that. Let's check out a description of the data:"]},{"cell_type":"code","metadata":{"id":"mpe2rdb3Hxh4"},"source":["df.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-K1cY18hLn06"},"source":["df.groupby('Sample_ID').BestAge.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R9gu1AKeJIQi"},"source":["df.groupby('Sample_ID').size() # number of rows per sample"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"32TnQYpAJTMH"},"source":["df.groupby('Sample_ID').BestAge.max() # oldest Age per sample (could also do min, mean, median, etc.)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VyK_wGnEL2-h"},"source":["df.BestAge.hist() # all ages as a histogram"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UAqiNHRtINLH"},"source":["# each sample as a separate box plot\n","df.groupby('Sample_ID').boxplot(column=['BestAge'], grid=False, showfliers=False, figsize=[10,20], sharey=True)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2fu9eHGQtLkr"},"source":["df.plot(x='BestAge', y='BestAge_err', style='.');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BLc7PdUn7w1E"},"source":["### Now you try\n","Make a plot using one of the built-in pandas plotting methods - anything you want! "]},{"cell_type":"code","metadata":{"id":"bbb1SGwE7w1E"},"source":["# your code goes here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"23I60nMAqZmD"},"source":["![I did it](https://media1.tenor.com/images/a5747f7b1d84287ca4a62e8a428d51ae/tenor.gif?itemid=4990241)"]},{"cell_type":"markdown","metadata":{"id":"ILqUaHPu6NTz"},"source":["# Some not-so-easy geochronology data\n","\n","Ok, now it's time for the not-so-easy dataset. Here goes:"]},{"cell_type":"code","metadata":{"id":"GcAjvaEy6NT8"},"source":["# these next two things shuoldnt need to be changed if you set up your Google Drive folder correctly (see Module 1)\n","folder_path = 'gdrive/My Drive/GEOL557_F22/data/' # makes a path\n","file_name = 'Daniels_GSA_2017304_appendix2.xlsx' # file name"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0QlgJFu26NUH"},"source":["## Let's load in a dataset\n","The data we want is located here https://doi.org/10.1130/B31757.1 It is the Appendix 2 file (\"Supplemental Information 2\" down at the bottom   of the   page). Two tabs in that xlsx are useful to us, the \"high throughput\" and \"low throughput\" tabs, which are U-Pb ages from detrital zircons from the Magallanes Basin, Chile. Also, by default, `pandas` imports the first sheet, but we also want to specify that, so we use `sheet_name` for that. \n","\n","Before we load the data, the Excel file is weird, and has a linked IsoPlot function in the file that needs to be broken/deleted. I took care of breaking this link and saving as a new file, which fixes weird encryption issues with Excel format. Yet another reason to just use a csv file... \n","\n","We load it with the pandas `read_xls` function into a DataFrame called `df`:"]},{"cell_type":"code","metadata":{"id":"5sbhaJoO6NUI"},"source":["df=pd.read_excel(folder_path + file_name, sheet_name='HighThroughputAges - LA-ICP-MS') # uses pandas to read in the csv as a 'DataFrame' called df\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d1NaaMoo6NUO"},"source":["Hmm, that doesnt look right. There are a few header lines, which are messing up the key names (currently in row `1`. You could go into Excel and delete them, but it's good practice to modify the input file as little as possible. Also, there are 13 samples and two tabs, so that means you would have to fix these issues 26 times manually, which is sub-optimal. Let's automate the boring stuff!"]},{"cell_type":"code","metadata":{"id":"ZNNkt0EJHjRM"},"source":["df=pd.read_excel(folder_path + file_name, sheet_name='HighThroughputAges - LA-ICP-MS', skiprows=2) # uses pandas to read in the csv as a 'DataFrame' called df\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dOrbPCpKIge1"},"source":["OK, that's a bit better, but still not great. If you look at the Excel, it has empty rows to separate the samples, and merged rows to indicate column names and subnames. This looks snazzy in Excel, but is a pain when doing data analysis because it means that one row is empty (hence the `Unnamed:1` as the second column name. \n","\n","First, let's get rid of the empty rows:"]},{"cell_type":"code","metadata":{"id":"b53-xymd6NUT"},"source":["print('df has length', len(df), 'before dropping rows with no data')\n","df.dropna(how='all', inplace=True)\n","print('df has length', len(df), 'after dropping rows with no data')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gYbKOSqx6NUV"},"source":["Now to deal with the column names. Ideally, we would rename the column names so that the name and the subname is in each, like this first example:"]},{"cell_type":"code","metadata":{"id":"rWY1Jyng63EY"},"source":["df.rename({'Data for Tera-Wasserburg plot2'\t: 'TeraWasserburg_238U/206Pb'},axis=1, inplace=True)\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xPyyZadTCFeR"},"source":["There are smarter, more automated ways to do that (using a loop or `.apply` methods), but that's a topic for another day. \n","\n","In this case, we don't really want to fool around with all those columns anyways - what we are really interested in are the ages, and so we can just drop all the columns that don't have age information. "]},{"cell_type":"code","metadata":{"id":"zqkRgqLrClyC"},"source":["#renames the columns first\n","col_dict = {'15-CC-01 HT':'Sample', \n","            'Unnamed: 1':'Spot', \n","            'Dates':'Age_207Pb/206Pb', \n","            'Unnamed: 25':'Age_206Pb/238U', \n","            'Unnamed: 28':'Age_207Pb/235Pb', \n","            'Accepted Dates4':'Age_Accepted'\n","           }\n","\n","df.rename(col_dict, axis=1, inplace=True) # axis 1 acts on the columns\n","\n","# now let's drop all the columns we didnt rename\n","col_list = list(col_dict.values())\n","df = df.filter(col_list, axis=1) # reassign it\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MuCadeVCgM_x"},"source":["# get rid of first line\n","df.drop(0,axis=0, inplace=True) # axis 0 works on the rows\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4RbMQyZFDsPH"},"source":["And lastly, from the way that xls was formatted, there should be some extra rows that only had values in the first column that we renamed to `Sample`, so let's look at that column:"]},{"cell_type":"code","metadata":{"id":"ch2WZAn9Df49"},"source":["print(df.Sample.value_counts())\n","print(df.Sample.value_counts().values)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DrHzV3bdFAEj"},"source":["Whoa, lots of weird things there. The way that xls is formatted is really not great! It would be smart to replace the sample names beginning with `UK` with the ones beginning with `15-`, but for now we won't mess with that. Let's just get rid of the rows that only have data in the `Sample` column, and no other data:"]},{"cell_type":"code","metadata":{"id":"zr6PKKYKE-57","scrolled":true},"source":["# Which columns have NaNs\n","df.isnull().sum()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7G6WG8Sg7w1a"},"source":["See the 32 in Spot? There shouldn't be any NaNs there either, if each `Spot` is part of a `Sample`. So, let's get rid of rows where Spot is NaN"]},{"cell_type":"code","metadata":{"id":"-QJiYhKqlvoO"},"source":["# get rid of all rows where Spot is NaN\n","df.dropna(axis=0, subset = ['Spot'], inplace=True)\n","\n","# now check the samples\n","print(df.isnull().sum())\n","print(df.Sample.value_counts())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RtsGx1F5njMv"},"source":["Great! That means we have 12 Samples, , with around 640 ages per sample. \n","\n","Last thing we need to do is to convert the `objects` to other dtypes:"]},{"cell_type":"code","metadata":{"id":"xLKh1WLWni1H"},"source":["df.info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eVjt9TbqDF60"},"source":["# get rid of strings in the age columns\n","df[df.columns[2:6]] = df[df.columns[2:6]].apply(pd.to_numeric, errors='coerce').fillna(0).astype(float).dropna()\n","# you can read about the coerce method on the pandas website"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gP_OJaaUniAB"},"source":["df.info()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A0phWFwdnJmC"},"source":["### Whew! \n","Ok, that took a bit of work, and you might think you can do this faster in Excel, and you are probably correct. but think about if instead of one spreadsheet, you had 55 or 550 of these spreadsheets to deal with - still want to do it in Excel? \n","\n","### Now let's try a simple plot:"]},{"cell_type":"code","metadata":{"id":"jmkfmMXdZrfz"},"source":["df.plot.scatter(x='Age_207Pb/206Pb', y='Age_206Pb/238U')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jGM2_tjlaXUi"},"source":["df['Age_206Pb/238U'].max()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SUBNX_XbD26R"},"source":["df.plot.scatter(x='Age_207Pb/206Pb', y='Age_206Pb/238U', xlim=[0, 4000], ylim=[0,4000])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C8M_vcNNEmHH"},"source":["### Now you try - for example, do a `describe()` on a column and a `hist` of ages (you can choose the column you want to plot)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yM5vDA8N7w1q"},"source":["# your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-i8rVb7O7w1r"},"source":["### Now let's export that clean dataset as a csv\n","Search the pandas documention to figure out how to export a csv of this dataset you have just cleaned up"]},{"cell_type":"code","metadata":{"id":"uc2wBmgz7w1s"},"source":["# your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"35lfh9tTqlmj"},"source":["![Robert Redford](https://media1.tenor.com/images/3952a85da6e63c7755607a40a4bc975f/tenor.gif?itemid=4959267)"]}]}